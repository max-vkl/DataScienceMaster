{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "from scipy import sparse\n",
    "from scipy.linalg import sqrtm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train = pd.read_csv('train.csv')\n",
    "complete_train_og = complete_train.copy()\n",
    "test = pd.read_csv('kaggle_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_70621/2786937693.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df2 = df2.groupby('user_id').apply(set_top_scores).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hercules (1997)</td>\n",
       "      <td>1566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>3408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanic (1997)</td>\n",
       "      <td>1721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>1193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800162</th>\n",
       "      <td>6040</td>\n",
       "      <td>Paris, Texas (1984)</td>\n",
       "      <td>1305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800163</th>\n",
       "      <td>6040</td>\n",
       "      <td>Breaking Away (1979)</td>\n",
       "      <td>3359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800164</th>\n",
       "      <td>6040</td>\n",
       "      <td>Simple Plan, A (1998)</td>\n",
       "      <td>2391</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800165</th>\n",
       "      <td>6040</td>\n",
       "      <td>Haunted World of Edward D. Wood Jr., The (1995)</td>\n",
       "      <td>722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800166</th>\n",
       "      <td>6040</td>\n",
       "      <td>Blues Brothers, The (1980)</td>\n",
       "      <td>1220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800167 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                                            title  movie_id  \\\n",
       "0             1                                  Hercules (1997)      1566   \n",
       "1             1                           Erin Brockovich (2000)      3408   \n",
       "2             1                                 Toy Story (1995)         1   \n",
       "3             1                                   Titanic (1997)      1721   \n",
       "4             1           One Flew Over the Cuckoo's Nest (1975)      1193   \n",
       "...         ...                                              ...       ...   \n",
       "800162     6040                              Paris, Texas (1984)      1305   \n",
       "800163     6040                             Breaking Away (1979)      3359   \n",
       "800164     6040                            Simple Plan, A (1998)      2391   \n",
       "800165     6040  Haunted World of Edward D. Wood Jr., The (1995)       722   \n",
       "800166     6040                       Blues Brothers, The (1980)      1220   \n",
       "\n",
       "        rating  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "800162       1  \n",
       "800163       1  \n",
       "800164       1  \n",
       "800165       0  \n",
       "800166       0  \n",
       "\n",
       "[800167 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = complete_train\n",
    "def set_top_scores(group):\n",
    "    # Find the highest rating\n",
    "    highest_rating = group['rating'].max()\n",
    "    if highest_rating == 5:\n",
    "        group['new_rating'] = group['rating'].apply(lambda x: 1 if x == highest_rating or x == highest_rating - 1 else 0)\n",
    "    else:\n",
    "        group['new_rating'] = group['rating'].apply(lambda x: 1 if x == highest_rating else 0)        \n",
    "    \n",
    "    return group\n",
    "\n",
    "df2 = df2.groupby('user_id').apply(set_top_scores).reset_index(drop=True)\n",
    "df2 = df2.drop(['rating'], axis=1)  # Remove original rating column\n",
    "df2.rename(columns={'new_rating': 'rating'}, inplace=True)\n",
    "df2 = df2.drop(['age', 'release_date', 'sex'], axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_70621/1299700687.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('user_id').apply(set_top_scores).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hercules (1997)</td>\n",
       "      <td>1566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>3408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanic (1997)</td>\n",
       "      <td>1721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>1193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800162</th>\n",
       "      <td>6040</td>\n",
       "      <td>Paris, Texas (1984)</td>\n",
       "      <td>1305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800163</th>\n",
       "      <td>6040</td>\n",
       "      <td>Breaking Away (1979)</td>\n",
       "      <td>3359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800164</th>\n",
       "      <td>6040</td>\n",
       "      <td>Simple Plan, A (1998)</td>\n",
       "      <td>2391</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800165</th>\n",
       "      <td>6040</td>\n",
       "      <td>Haunted World of Edward D. Wood Jr., The (1995)</td>\n",
       "      <td>722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800166</th>\n",
       "      <td>6040</td>\n",
       "      <td>Blues Brothers, The (1980)</td>\n",
       "      <td>1220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800167 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                                            title  movie_id  \\\n",
       "0             1                                  Hercules (1997)      1566   \n",
       "1             1                           Erin Brockovich (2000)      3408   \n",
       "2             1                                 Toy Story (1995)         1   \n",
       "3             1                                   Titanic (1997)      1721   \n",
       "4             1           One Flew Over the Cuckoo's Nest (1975)      1193   \n",
       "...         ...                                              ...       ...   \n",
       "800162     6040                              Paris, Texas (1984)      1305   \n",
       "800163     6040                             Breaking Away (1979)      3359   \n",
       "800164     6040                            Simple Plan, A (1998)      2391   \n",
       "800165     6040  Haunted World of Edward D. Wood Jr., The (1995)       722   \n",
       "800166     6040                       Blues Brothers, The (1980)      1220   \n",
       "\n",
       "        rating  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "800162       1  \n",
       "800163       1  \n",
       "800164       1  \n",
       "800165       0  \n",
       "800166       0  \n",
       "\n",
       "[800167 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = complete_train\n",
    "def set_top_scores(group):\n",
    "    # Find the highest rating\n",
    "    highest_rating = group['rating'].max()\n",
    "    if highest_rating == 5 or highest_rating == 4:\n",
    "        group['new_rating'] = group['rating'].apply(lambda x: 1 if x == highest_rating or x == highest_rating - 1 else 0)\n",
    "    else:\n",
    "        group['new_rating'] = group['rating'].apply(lambda x: 1 if x == highest_rating else 0)        \n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby('user_id').apply(set_top_scores).reset_index(drop=True)\n",
    "df = df.drop(['rating'], axis=1)  # Remove original rating column\n",
    "df.rename(columns={'new_rating': 'rating'}, inplace=True)\n",
    "df = df.drop(['age', 'release_date', 'sex'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigene Umwandlung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_68321/3033812230.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  um_pairs['rating'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hercules (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Titanic (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22227195</th>\n",
       "      <td>6040</td>\n",
       "      <td>2703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Broken Vessels (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22227196</th>\n",
       "      <td>6040</td>\n",
       "      <td>2845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>White Boys (1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22227197</th>\n",
       "      <td>6040</td>\n",
       "      <td>3607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>One Little Indian (1973)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22227198</th>\n",
       "      <td>6040</td>\n",
       "      <td>1360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Identification of a Woman (Identificazione di ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22227199</th>\n",
       "      <td>6040</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neon Bible, The (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22227200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  movie_id  rating  \\\n",
       "0               1      1566     1.0   \n",
       "1               1      3408     1.0   \n",
       "2               1         1     1.0   \n",
       "3               1      1721     1.0   \n",
       "4               1      1193     1.0   \n",
       "...           ...       ...     ...   \n",
       "22227195     6040      2703     0.0   \n",
       "22227196     6040      2845     0.0   \n",
       "22227197     6040      3607     0.0   \n",
       "22227198     6040      1360     0.0   \n",
       "22227199     6040       138     0.0   \n",
       "\n",
       "                                                      title  \n",
       "0                                           Hercules (1997)  \n",
       "1                                    Erin Brockovich (2000)  \n",
       "2                                          Toy Story (1995)  \n",
       "3                                            Titanic (1997)  \n",
       "4                    One Flew Over the Cuckoo's Nest (1975)  \n",
       "...                                                     ...  \n",
       "22227195                              Broken Vessels (1998)  \n",
       "22227196                                  White Boys (1999)  \n",
       "22227197                           One Little Indian (1973)  \n",
       "22227198  Identification of a Woman (Identificazione di ...  \n",
       "22227199                             Neon Bible, The (1995)  \n",
       "\n",
       "[22227200 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movie_title_lookup = df[['movie_id', 'title']].drop_duplicates().set_index('movie_id')['title'].to_dict()\n",
    "\n",
    "# uids = df['user_id'].unique()\n",
    "# mids = df['movie_id'].unique()\n",
    "# um_pairs = pd.DataFrame([(uid, mid) for uid in uids for mid in mids], columns=['user_id', 'movie_id'])\n",
    "\n",
    "# um_pairs = um_pairs.merge(df[['user_id', 'movie_id', 'rating']], on=['user_id', 'movie_id'], how='left')\n",
    "# um_pairs['title'] = um_pairs['movie_id'].map(movie_title_lookup).fillna(0)\n",
    "# um_pairs['rating'].fillna(0, inplace=True)\n",
    "# df = um_pairs\n",
    "# um_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.01, random_state=7)\n",
    "\n",
    "x_train = train[[\"user_id\", \"movie_id\"]].values\n",
    "y_train = train[\"rating\"].values\n",
    "\n",
    "x_val = val[[\"user_id\", \"movie_id\"]].values\n",
    "y_val = val[\"rating\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train2, val2 = train_test_split(df2, test_size=0.01, random_state=7)\n",
    "\n",
    "x_train2 = train2[[\"user_id\", \"movie_id\"]].values\n",
    "y_train2 = train2[\"rating\"].values\n",
    "\n",
    "x_val2 = val2[[\"user_id\", \"movie_id\"]].values\n",
    "y_val2 = val2[\"rating\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 6040, Number of Movies: 3680, Min rating: 0.0, Max rating: 1.0\n"
     ]
    }
   ],
   "source": [
    "user_ids = df[\"user_id\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "movie_ids = df[\"movie_id\"].unique().tolist()\n",
    "movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n",
    "movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n",
    "df[\"user\"] = df[\"user_id\"].map(user2user_encoded)\n",
    "df[\"movie\"] = df[\"movie_id\"].map(movie2movie_encoded)\n",
    "\n",
    "num_users = len(user2user_encoded)\n",
    "num_movies = len(movie_encoded2movie)\n",
    "df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n",
    "# min and max ratings will be used to normalize the ratings later\n",
    "min_rating = min(df[\"rating\"])\n",
    "max_rating = max(df[\"rating\"])\n",
    "\n",
    "print(\n",
    "    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n",
    "        num_users, num_movies, min_rating, max_rating\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(history):\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "    plt.axis([0,len(history.history[\"loss\"]),np.min(history.history[\"loss\"]),np.max(history.history[\"val_loss\"])])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(predict_f,data_train,data_test):\n",
    "    \"\"\" RMSE-based predictive performance evaluation with pandas. \"\"\"\n",
    "    ids_to_estimate = zip(data_test.user_id, data_test.movie_id)\n",
    "    list_users = set(data_train.user_id)\n",
    "    estimated = np.array([predict_f(u,i) if u in list_users else 3 for (u,i) in ids_to_estimate ])\n",
    "    real = data_test.rating.values\n",
    "    return compute_rmse(estimated, real)\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):  \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def AP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    ap_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return ap_score\n",
    "\n",
    "def evaluate_algorithm_top(test, recommender_object, at=25, thr_relevant = 0.85):\n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_AP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    for user_id in tqdm(test.user_id.unique()):\n",
    "        \n",
    "        relevant_items = test[test.user_id==user_id]\n",
    "        thr = np.quantile(relevant_items.rating,thr_relevant)\n",
    "        relevant_items = np.array(relevant_items[relevant_items.rating >=thr].movie_id.values)\n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = top_recomendations(recommender_object,user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_AP += AP(recommended_items, relevant_items)\n",
    "            \n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    MAP = cumulative_AP / num_eval\n",
    "    \n",
    "    print(\"Recommender results are: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, MAP)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "class RecommenderNetV(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(RecommenderNetV, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_movie_embedding = layers.Embedding(\n",
    "            num_users+num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_movie_embedding(inputs[:, 0])\n",
    "        movie_vector = self.user_movie_embedding(inputs[:, 1]+num_users)\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class LogisticRecommenderNet(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(LogisticRecommenderNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        # Embeddings for users and movies\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        # Biases for users and movies\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        # Dot product of user and movie vectors + user and movie biases\n",
    "        dot_user_movie = tf.reduce_sum(user_vector * movie_vector, axis=1, keepdims=True)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        # Apply sigmoid activation to model probability\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_recomendations(model_object, user_id,at = 25):\n",
    "    movies_watched_by_user = df[df.user_id == user_id]\n",
    "\n",
    "    movies_not_watched = df[df[\"movie_id\"].isin(movies_watched_by_user.movie_id.values)][\"movie_id\"]\n",
    "    movies_not_watched = list(set(movies_not_watched).intersection(set(movie2movie_encoded.keys())))\n",
    "    movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n",
    "\n",
    "    \n",
    "    user_encoder = user2user_encoded.get(user_id)\n",
    "    user_movie_array = np.hstack(([[user_encoder]] * len(movies_not_watched), movies_not_watched))\n",
    "\n",
    "    ratings = model_object.predict(user_movie_array,verbose=0).flatten()\n",
    "    top_ratings_indices = ratings.argsort()[-at:][::-1]\n",
    "    recommended_movie_ids = [movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices]\n",
    "    return recommended_movie_ids, ratings[top_ratings_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 887us/step - loss: 0.3071 - val_loss: 0.1914\n",
      "Epoch 2/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 865us/step - loss: 0.1895 - val_loss: 0.1889\n",
      "Epoch 3/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 964us/step - loss: 0.1871 - val_loss: 0.1891\n",
      "Epoch 4/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 935us/step - loss: 0.1877 - val_loss: 0.1909\n",
      "Epoch 5/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 0.1880 - val_loss: 0.1905\n",
      "Epoch 6/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 885us/step - loss: 0.1880 - val_loss: 0.1919\n",
      "Epoch 7/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 904us/step - loss: 0.1886 - val_loss: 0.1914\n",
      "Epoch 8/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 801us/step - loss: 0.1882 - val_loss: 0.1924\n",
      "Epoch 9/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 804us/step - loss: 0.1889 - val_loss: 0.1902\n",
      "Epoch 10/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 806us/step - loss: 0.1889 - val_loss: 0.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3340/3340 [02:49<00:00, 19.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender results are: Precision = 0.0147, Recall = 0.2746, MAP = 0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mf_model_start = RecommenderNetV(num_users, num_movies, EMBEDDING_SIZE)\n",
    "mf_model_start.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
    ")\n",
    "mf_history = mf_model_start.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "evaluate_algorithm_top(val, mf_model_start, at = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 907us/step - loss: 0.2653 - val_loss: 0.2069\n",
      "Epoch 2/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 988us/step - loss: 0.2095 - val_loss: 0.2147\n",
      "Epoch 3/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 962us/step - loss: 0.2184 - val_loss: 0.2141\n",
      "Epoch 4/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 978us/step - loss: 0.2260 - val_loss: 0.2210\n",
      "Epoch 5/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 967us/step - loss: 0.2302 - val_loss: 0.2451\n",
      "Epoch 6/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 951us/step - loss: 0.2363 - val_loss: 0.2390\n",
      "Epoch 7/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.2349 - val_loss: 0.2326\n",
      "Epoch 8/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 861us/step - loss: 0.2383 - val_loss: 0.2253\n",
      "Epoch 9/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 971us/step - loss: 0.2395 - val_loss: 0.2261\n",
      "Epoch 10/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 895us/step - loss: 0.2431 - val_loss: 0.2517\n",
      "Epoch 11/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 934us/step - loss: 0.2399 - val_loss: 0.2483\n",
      "Epoch 12/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 870us/step - loss: 0.2405 - val_loss: 0.2370\n",
      "Epoch 13/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 901us/step - loss: 0.2431 - val_loss: 0.2376\n",
      "Epoch 14/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 951us/step - loss: 0.2427 - val_loss: 0.2330\n",
      "Epoch 15/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 0.2415 - val_loss: 0.2468\n",
      "Epoch 16/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 817us/step - loss: 0.2443 - val_loss: 0.2369\n",
      "Epoch 17/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 830us/step - loss: 0.2444 - val_loss: 0.2218\n",
      "Epoch 18/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 845us/step - loss: 0.2406 - val_loss: 0.2346\n",
      "Epoch 19/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 819us/step - loss: 0.2423 - val_loss: 0.2489\n",
      "Epoch 20/20\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 820us/step - loss: 0.2441 - val_loss: 0.2391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3340/3340 [02:50<00:00, 19.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender results are: Precision = 0.0149, Recall = 0.2757, MAP = 0.0479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mf_model_start1 = RecommenderNetV(num_users, num_movies, EMBEDDING_SIZE)\n",
    "mf_model_start1.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
    ")\n",
    "mf_history = mf_model_start1.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "evaluate_algorithm_top(val, mf_model_start1, at = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 835us/step - loss: 0.1880 - val_loss: 0.1917\n",
      "Epoch 2/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 957us/step - loss: 0.1880 - val_loss: 0.1911\n",
      "Epoch 3/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 929us/step - loss: 0.1886 - val_loss: 0.1917\n",
      "Epoch 4/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 907us/step - loss: 0.1885 - val_loss: 0.1934\n",
      "Epoch 5/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 819us/step - loss: 0.1889 - val_loss: 0.1912\n",
      "Epoch 6/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 810us/step - loss: 0.1889 - val_loss: 0.1919\n",
      "Epoch 7/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 961us/step - loss: 0.1884 - val_loss: 0.1923\n",
      "Epoch 8/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.1889 - val_loss: 0.1916\n",
      "Epoch 9/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.1885 - val_loss: 0.1935\n",
      "Epoch 10/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 972us/step - loss: 0.1888 - val_loss: 0.1909\n"
     ]
    }
   ],
   "source": [
    "mf_model_start2 = RecommenderNetV(num_users, num_movies, EMBEDDING_SIZE)\n",
    "mf_model_start.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
    ")\n",
    "mf_history = mf_model_start.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "#evaluate_algorithm_top(val, mf_model_start, at = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3340/3340 [02:50<00:00, 19.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender results are: Precision = 0.0153, Recall = 0.2865, MAP = 0.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm_top(val, mf_model_start2, at = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 688us/step - loss: 0.2358 - val_loss: 0.1999\n",
      "Epoch 2/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 676us/step - loss: 0.1944 - val_loss: 0.1896\n",
      "Epoch 3/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 713us/step - loss: 0.1857 - val_loss: 0.1867\n",
      "Epoch 4/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 796us/step - loss: 0.1823 - val_loss: 0.1853\n",
      "Epoch 5/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 787us/step - loss: 0.1802 - val_loss: 0.1844\n",
      "Epoch 6/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 688us/step - loss: 0.1782 - val_loss: 0.1837\n",
      "Epoch 7/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 683us/step - loss: 0.1772 - val_loss: 0.1833\n",
      "Epoch 8/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 846us/step - loss: 0.1758 - val_loss: 0.1830\n",
      "Epoch 9/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 689us/step - loss: 0.1747 - val_loss: 0.1826\n",
      "Epoch 10/10\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 934us/step - loss: 0.1735 - val_loss: 0.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3340/3340 [02:57<00:00, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender results are: Precision = 0.0146, Recall = 0.2717, MAP = 0.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mf_model_new = LogisticRecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "mf_model_new.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
    ")\n",
    "mf_history = mf_model_new.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "evaluate_algorithm_top(val, mf_model_new, at = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 686us/step - loss: 0.2363 - val_loss: 0.2007\n",
      "Epoch 2/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 684us/step - loss: 0.1950 - val_loss: 0.1892\n",
      "Epoch 3/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 871us/step - loss: 0.1852 - val_loss: 0.1863\n",
      "Epoch 4/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 848us/step - loss: 0.1821 - val_loss: 0.1850\n",
      "Epoch 5/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 908us/step - loss: 0.1797 - val_loss: 0.1840\n",
      "Epoch 6/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 788us/step - loss: 0.1785 - val_loss: 0.1836\n",
      "Epoch 7/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 766us/step - loss: 0.1769 - val_loss: 0.1833\n",
      "Epoch 8/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 703us/step - loss: 0.1755 - val_loss: 0.1830\n",
      "Epoch 9/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.1746 - val_loss: 0.1827\n",
      "Epoch 10/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 0.1739 - val_loss: 0.1824\n",
      "Epoch 11/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 767us/step - loss: 0.1727 - val_loss: 0.1821\n",
      "Epoch 12/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 891us/step - loss: 0.1718 - val_loss: 0.1819\n",
      "Epoch 13/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 958us/step - loss: 0.1705 - val_loss: 0.1816\n",
      "Epoch 14/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.1695 - val_loss: 0.1815\n",
      "Epoch 15/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 854us/step - loss: 0.1685 - val_loss: 0.1814\n",
      "Epoch 16/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 927us/step - loss: 0.1675 - val_loss: 0.1814\n",
      "Epoch 17/17\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 850us/step - loss: 0.1664 - val_loss: 0.1815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3340/3340 [03:03<00:00, 18.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender results are: Precision = 0.0147, Recall = 0.2747, MAP = 0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mf_model_new = LogisticRecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "mf_model_new.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
    ")\n",
    "mf_history = mf_model_new.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs= 17,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "evaluate_algorithm_top(val, mf_model_new, at = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 699us/step - loss: 0.2471 - val_loss: 0.2379\n",
      "Epoch 2/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 880us/step - loss: 0.2353 - val_loss: 0.2265\n",
      "Epoch 3/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 782us/step - loss: 0.2237 - val_loss: 0.2157\n",
      "Epoch 4/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 711us/step - loss: 0.2129 - val_loss: 0.2072\n",
      "Epoch 5/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 743us/step - loss: 0.2044 - val_loss: 0.2011\n",
      "Epoch 6/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 652us/step - loss: 0.1984 - val_loss: 0.1969\n",
      "Epoch 7/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 0.1938 - val_loss: 0.1940\n",
      "Epoch 8/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 752us/step - loss: 0.1908 - val_loss: 0.1920\n",
      "Epoch 9/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 730us/step - loss: 0.1884 - val_loss: 0.1905\n",
      "Epoch 10/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 638us/step - loss: 0.1863 - val_loss: 0.1894\n",
      "Epoch 11/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 721us/step - loss: 0.1851 - val_loss: 0.1886\n",
      "Epoch 12/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 650us/step - loss: 0.1844 - val_loss: 0.1879\n",
      "Epoch 13/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 653us/step - loss: 0.1839 - val_loss: 0.1874\n",
      "Epoch 14/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 689us/step - loss: 0.1826 - val_loss: 0.1869\n",
      "Epoch 15/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 713us/step - loss: 0.1818 - val_loss: 0.1865\n",
      "Epoch 16/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 672us/step - loss: 0.1816 - val_loss: 0.1862\n",
      "Epoch 17/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 660us/step - loss: 0.1811 - val_loss: 0.1859\n",
      "Epoch 18/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 696us/step - loss: 0.1804 - val_loss: 0.1856\n",
      "Epoch 19/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 822us/step - loss: 0.1803 - val_loss: 0.1853\n",
      "Epoch 20/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 721us/step - loss: 0.1795 - val_loss: 0.1851\n",
      "Epoch 21/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 737us/step - loss: 0.1796 - val_loss: 0.1849\n",
      "Epoch 22/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 642us/step - loss: 0.1789 - val_loss: 0.1847\n",
      "Epoch 23/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 718us/step - loss: 0.1785 - val_loss: 0.1845\n",
      "Epoch 24/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 660us/step - loss: 0.1782 - val_loss: 0.1844\n",
      "Epoch 25/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 619us/step - loss: 0.1778 - val_loss: 0.1842\n",
      "Epoch 26/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 629us/step - loss: 0.1775 - val_loss: 0.1841\n",
      "Epoch 27/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 606us/step - loss: 0.1776 - val_loss: 0.1840\n",
      "Epoch 28/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 634us/step - loss: 0.1770 - val_loss: 0.1839\n",
      "Epoch 29/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 676us/step - loss: 0.1765 - val_loss: 0.1838\n",
      "Epoch 30/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 768us/step - loss: 0.1767 - val_loss: 0.1837\n",
      "Epoch 31/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 816us/step - loss: 0.1763 - val_loss: 0.1836\n",
      "Epoch 32/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 692us/step - loss: 0.1763 - val_loss: 0.1836\n",
      "Epoch 33/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 680us/step - loss: 0.1758 - val_loss: 0.1835\n",
      "Epoch 34/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 709us/step - loss: 0.1752 - val_loss: 0.1834\n",
      "Epoch 35/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 767us/step - loss: 0.1750 - val_loss: 0.1834\n",
      "Epoch 36/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 796us/step - loss: 0.1752 - val_loss: 0.1833\n",
      "Epoch 37/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 672us/step - loss: 0.1746 - val_loss: 0.1832\n",
      "Epoch 38/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 673us/step - loss: 0.1745 - val_loss: 0.1832\n",
      "Epoch 39/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 698us/step - loss: 0.1744 - val_loss: 0.1831\n",
      "Epoch 40/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 653us/step - loss: 0.1735 - val_loss: 0.1830\n",
      "Epoch 41/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 648us/step - loss: 0.1741 - val_loss: 0.1830\n",
      "Epoch 42/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 976us/step - loss: 0.1738 - val_loss: 0.1829\n",
      "Epoch 43/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 666us/step - loss: 0.1731 - val_loss: 0.1828\n",
      "Epoch 44/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 601us/step - loss: 0.1729 - val_loss: 0.1828\n",
      "Epoch 45/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 719us/step - loss: 0.1729 - val_loss: 0.1827\n",
      "Epoch 46/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 750us/step - loss: 0.1727 - val_loss: 0.1826\n",
      "Epoch 47/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 777us/step - loss: 0.1727 - val_loss: 0.1826\n",
      "Epoch 48/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 0.1723 - val_loss: 0.1825\n",
      "Epoch 49/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 0.1723 - val_loss: 0.1825\n",
      "Epoch 50/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 805us/step - loss: 0.1720 - val_loss: 0.1824\n",
      "Epoch 51/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 926us/step - loss: 0.1717 - val_loss: 0.1824\n",
      "Epoch 52/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 624us/step - loss: 0.1717 - val_loss: 0.1823\n",
      "Epoch 53/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 648us/step - loss: 0.1713 - val_loss: 0.1823\n",
      "Epoch 54/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 727us/step - loss: 0.1712 - val_loss: 0.1822\n",
      "Epoch 55/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 764us/step - loss: 0.1710 - val_loss: 0.1822\n",
      "Epoch 56/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 632us/step - loss: 0.1708 - val_loss: 0.1821\n",
      "Epoch 57/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 658us/step - loss: 0.1705 - val_loss: 0.1821\n",
      "Epoch 58/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 665us/step - loss: 0.1703 - val_loss: 0.1821\n",
      "Epoch 59/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 930us/step - loss: 0.1702 - val_loss: 0.1820\n",
      "Epoch 60/60\n",
      "\u001b[1m6189/6189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 845us/step - loss: 0.1702 - val_loss: 0.1820\n"
     ]
    }
   ],
   "source": [
    "mf_model_new3 = LogisticRecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "mf_model_new3.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(), \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001)\n",
    ")\n",
    "mf_history = mf_model_new3.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=60,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "#evaluate_algorithm_top(val, mf_model_new3, at = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3340 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3340/3340 [03:04<00:00, 18.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender results are: Precision = 0.0074, Recall = 0.2733, MAP = 0.0457\n",
      "([596, 380, 2973, 377, 1201, 2085, 594, 1029, 1023, 2080, 2700, 2194, 2137, 2087, 2804, 3702, 2366, 2353, 1287, 1104, 595, 1954, 2687, 1233, 3153], array([0.91154355, 0.86413103, 0.8373187 , 0.7916293 , 0.7250679 ,\n",
      "       0.70799524, 0.701083  , 0.5668616 , 0.5660668 , 0.5440038 ,\n",
      "       0.53373253, 0.5200685 , 0.51977426, 0.51509374, 0.4999377 ,\n",
      "       0.48826393, 0.47459283, 0.45716706, 0.45703134, 0.44794562,\n",
      "       0.44706926, 0.42869732, 0.41613147, 0.4010609 , 0.38597077],\n",
      "      dtype=float32))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm_top(val, mf_model_new3, at = 25)\n",
    "print(top_recomendations(mf_model_new3, 1000, at = 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:34:56.042181: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 1 of dimension 1 out of bounds.\n",
      "2024-03-29 16:34:56.042199: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: slice index 1 of dimension 1 out of bounds.\n",
      "\t [[{{node logistic_recommender_net_1_1/strided_slice_1}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node logistic_recommender_net_1_1/strided_slice_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_68321/367191688.py\", line 1, in <module>\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 515, in predict\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 213, in one_step_on_data_distributed\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 202, in one_step_on_data\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 96, in predict_step\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/layer.py\", line 814, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/ops/operation.py\", line 48, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_68321/3122574160.py\", line 54, in call\n\nslice index 1 of dimension 1 out of bounds.\n\t [[{{node logistic_recommender_net_1_1/strided_slice_1}}]] [Op:__inference_one_step_on_data_distributed_384301]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m movie_encoded \u001b[38;5;241m=\u001b[39m movie2movie_encoded\u001b[38;5;241m.\u001b[39mget(movie_id)\n\u001b[1;32m      5\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[user_encoded], [movie_encoded \u001b[38;5;241m+\u001b[39m num_users]])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmf_model_new3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node logistic_recommender_net_1_1/strided_slice_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/max/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_68321/367191688.py\", line 1, in <module>\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 515, in predict\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 213, in one_step_on_data_distributed\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 202, in one_step_on_data\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 96, in predict_step\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/layer.py\", line 814, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/ops/operation.py\", line 48, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_68321/3122574160.py\", line 54, in call\n\nslice index 1 of dimension 1 out of bounds.\n\t [[{{node logistic_recommender_net_1_1/strided_slice_1}}]] [Op:__inference_one_step_on_data_distributed_384301]"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6037 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6037/6037 [3:15:24<00:00,  2.03s/it]  "
     ]
    }
   ],
   "source": [
    "from tqdm.std import tqdm\n",
    "import csv\n",
    "import os\n",
    "os.environ[\"TQDM_CONSOLE\"] = \"True\"\n",
    "\n",
    "with open('finalAttempt2.csv', 'w',encoding='UTF8') as f:\n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "    # write a row to the csv file\n",
    "    writer.writerow(['user_id', 'prediction'])\n",
    "    pbar = tqdm(total=len(test.user_id.unique()))\n",
    "    for user_id in tqdm(test.user_id.unique(),leave=False):\n",
    "        #n_ratings = ready_df[ready_df['uid']==user_id].movie_id.count()\n",
    "        #if(n_ratings > 30):\n",
    "        relevant_items = top_recomendations(mf_model, user_id,at = 25)\n",
    "        #else:\n",
    "        # relevant_items = topPopular.predict_top(user_id, at=25)\n",
    "        list_relevants = ' '.join([str(elem) for elem in relevant_items])\n",
    "        writer.writerow([str(user_id),list_relevants])\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 314/6040 [10:11<3:18:20,  2.08s/it]"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm_top(val, mf_model, at = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dieser Github Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'movie_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'movie_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovie_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'movie_id'"
     ]
    }
   ],
   "source": [
    "train = train.copy()\n",
    "val = val.copy()\n",
    "\n",
    "y_train = train['rating']\n",
    "train.drop(['rating'], axis=1, inplace=True)\n",
    "\n",
    "train['user_id'] = train['user_id'].astype('str')\n",
    "train['movie_id'] = train['movie_id'].astype('str')\n",
    "\n",
    "y_val = val[\"rating\"]\n",
    "val.drop(['rating'], axis=1, inplace=True)\n",
    "val['user_id'] = val['user_id'].astype('str')\n",
    "val['movie_id'] = val['movie_id'].astype('str')\n",
    "\n",
    "test['user_id'] = test['user_id'].astype('str')\n",
    "test['movie_id'] = test['movie_id'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notizen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normale Ratings, 1 Epoche, Batch_size 128: 0.9 (RMSE) 0.0933 (MAP)\n",
    "\n",
    "\n",
    "- Normale Rating, mit Implicit, 4 E, Batch 128: 0.4752 val los, Kaggle 0.00182"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
