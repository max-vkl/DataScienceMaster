{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "from scipy import sparse\n",
    "from scipy.linalg import sqrtm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train = pd.read_csv('train.csv')\n",
    "complete_train_og = complete_train.copy()\n",
    "test = pd.read_csv('kaggle_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/mm0hrmlx3g9dv10c735753m00000gn/T/ipykernel_15887/2104233964.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('user_id').apply(only_the_best).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hercules (1997)</td>\n",
       "      <td>1566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>3408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanic (1997)</td>\n",
       "      <td>1721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>1193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800162</th>\n",
       "      <td>6040</td>\n",
       "      <td>Paris, Texas (1984)</td>\n",
       "      <td>1305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800163</th>\n",
       "      <td>6040</td>\n",
       "      <td>Breaking Away (1979)</td>\n",
       "      <td>3359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800164</th>\n",
       "      <td>6040</td>\n",
       "      <td>Simple Plan, A (1998)</td>\n",
       "      <td>2391</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800165</th>\n",
       "      <td>6040</td>\n",
       "      <td>Haunted World of Edward D. Wood Jr., The (1995)</td>\n",
       "      <td>722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800166</th>\n",
       "      <td>6040</td>\n",
       "      <td>Blues Brothers, The (1980)</td>\n",
       "      <td>1220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800167 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                                            title  movie_id  \\\n",
       "0             1                                  Hercules (1997)      1566   \n",
       "1             1                           Erin Brockovich (2000)      3408   \n",
       "2             1                                 Toy Story (1995)         1   \n",
       "3             1                                   Titanic (1997)      1721   \n",
       "4             1           One Flew Over the Cuckoo's Nest (1975)      1193   \n",
       "...         ...                                              ...       ...   \n",
       "800162     6040                              Paris, Texas (1984)      1305   \n",
       "800163     6040                             Breaking Away (1979)      3359   \n",
       "800164     6040                            Simple Plan, A (1998)      2391   \n",
       "800165     6040  Haunted World of Edward D. Wood Jr., The (1995)       722   \n",
       "800166     6040                       Blues Brothers, The (1980)      1220   \n",
       "\n",
       "        rating  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "800162       1  \n",
       "800163       1  \n",
       "800164       1  \n",
       "800165       0  \n",
       "800166       0  \n",
       "\n",
       "[800167 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = complete_train\n",
    "def set_top_scores(group):\n",
    "    # Find the highest rating\n",
    "    highest_rating = group['rating'].max()\n",
    "    if highest_rating == 5 or highest_rating == 4:\n",
    "        group['new_rating'] = group['rating'].apply(lambda x: 1 if x == highest_rating or x == highest_rating - 1 else 0)\n",
    "    else:\n",
    "        group['new_rating'] = group['rating'].apply(lambda x: 1 if x == highest_rating else 0)        \n",
    "    \n",
    "    return group\n",
    "\n",
    "def only_the_best(group):\n",
    "    # Find the highest rating\n",
    "    highest_rating = group['rating'].max()\n",
    "    group['new_rating'] = group['rating'].apply(lambda x: 1 if x == 5 or x == 4 else 0) \n",
    "    \n",
    "    return group\n",
    "\n",
    "df = df.groupby('user_id').apply(only_the_best).reset_index(drop=True)\n",
    "df = df.drop(['rating'], axis=1)  # Remove original rating column\n",
    "df.rename(columns={'new_rating': 'rating'}, inplace=True)\n",
    "df = df.drop(['age', 'release_date', 'sex'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=7)\n",
    "\n",
    "train = train.copy()\n",
    "val = val.copy()\n",
    "\n",
    "y_train = train['rating']\n",
    "train.drop(['rating'], axis=1, inplace=True)\n",
    "\n",
    "train['user_id'] = train['user_id'].astype('str')\n",
    "train['movie_id'] = train['movie_id'].astype('str')\n",
    "\n",
    "y_val = val[\"rating\"]\n",
    "val.drop(['rating'], axis=1, inplace=True)\n",
    "val['user_id'] = val['user_id'].astype('str')\n",
    "val['movie_id'] = val['movie_id'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_f,data_train,data_test):\n",
    "    \"\"\" RMSE-based predictive performance evaluation with pandas. \"\"\"\n",
    "    ids_to_estimate = zip(data_test.user_id, data_test.movie_id)\n",
    "    list_users = set(data_train.user_id)\n",
    "    estimated = np.array([predict_f(u,i) if u in list_users else 3 for (u,i) in ids_to_estimate ])\n",
    "    real = data_test.rating.values\n",
    "    return compute_rmse(estimated, real)\n",
    "\n",
    "def compute_rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):  \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def AP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    ap_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return ap_score\n",
    "\n",
    "def evaluate_algorithm_top(test, recommender_object, at=25, thr_relevant = 0.85):\n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_AP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    for user_id in tqdm(test.user_id.unique()):\n",
    "        \n",
    "        relevant_items = test[test.user_id==user_id]\n",
    "        thr = np.quantile(relevant_items.rating,thr_relevant)\n",
    "        relevant_items = np.array(relevant_items[relevant_items.rating >=thr].movie_id.values)\n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommender_object.predict_top(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_AP += AP(recommended_items, relevant_items)\n",
    "            \n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    MAP = cumulative_AP / num_eval\n",
    "    \n",
    "    print(\"Recommender results are: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, MAP)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class FM():\n",
    "    def __init__(self,k = 40,learning_rate = 10,solver='stochastic',iterations = 100,regularizer = 0.00035,verbose = True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.solver=solver\n",
    "        self.k = k\n",
    "        self.iterations = iterations\n",
    "        self.indices = {}\n",
    "        self.regularizer = regularizer\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "    def set_indices(self,X):\n",
    "        i = 0\n",
    "        cols = {}\n",
    "        for col in X.columns:\n",
    "            cols[col] = i\n",
    "            i += 1\n",
    "\n",
    "        indices = {}\n",
    "        nz = 0\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object':\n",
    "                indices[cols[col]] = {}\n",
    "                colset = set(X[col])\n",
    "                for a in colset:\n",
    "                    indices[cols[col]][a] = nz\n",
    "                    nz += 1\n",
    "            else:\n",
    "                indices[cols[col]] = nz\n",
    "                nz += 1\n",
    "        self.indices = indices\n",
    "        self.nz = nz\n",
    "        \n",
    "    \n",
    "    def getIndexVal(self, col, val):\n",
    "        if isinstance(self.indices[col], int) or isinstance(self.indices[col], float):\n",
    "            return self.indices[col], val\n",
    "        else:\n",
    "            try:\n",
    "                return self.indices[col][val], 1.\n",
    "            except:\n",
    "                #print(col, val)\n",
    "                return 0., 0.\n",
    "            \n",
    "    def getIndexValArray(self, arr):\n",
    "        out = []\n",
    "        for i in range(len(arr)):\n",
    "            out.append(self.getIndexVal(i, arr[i]))\n",
    "        return out\n",
    "    \n",
    "    def initialize_params(self, X, y):\n",
    "        self.set_indices(X)\n",
    "        self.w0 = np.mean(y)\n",
    "        self.w1 = np.zeros(self.nz)\n",
    "        self.v = np.random.normal(scale=0.1,size=(self.nz, self.k))\n",
    "        \n",
    "    def sgd(self, X, y, verbose = True):\n",
    "\n",
    "        learning_rate = self.learning_rate\n",
    "        regularizer = self.regularizer\n",
    "        w0 = self.w0\n",
    "        w1 = self.w1\n",
    "        v = self.v\n",
    "        nz = self.nz\n",
    "\n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        #SGD\n",
    "        for epoch in range(self.iterations):\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            preds = []\n",
    "            for i in range(m):\n",
    "\n",
    "                _x = X[i, :]\n",
    "                _ivals = self.getIndexValArray(_x)\n",
    "                ind = {}\n",
    "                sum1 = 0\n",
    "                sum2 = 0\n",
    "                sum3 = np.zeros(self.k)\n",
    "\n",
    "                for col in range(n):\n",
    "                    index, val = _ivals[col]\n",
    "                    ind[index] = val\n",
    "                    sum1 += w1[index] * val\n",
    "\n",
    "                for f in range(self.k):\n",
    "                    s1 = 0.0\n",
    "                    s2 = 0.0\n",
    "                    for col in range(n):\n",
    "                        index, val = _ivals[col]\n",
    "                        temp = v[index, f] * val\n",
    "                        s1 += temp\n",
    "                        s2 += temp*temp\n",
    "                    sum3[f] = s1\n",
    "                    sum2 += s1*s1 - s2\n",
    "\n",
    "                y_hat = w0 + sum1 + 0.5*sum2\n",
    "                y_hat = max(1., y_hat)\n",
    "                y_hat = min(5., y_hat)\n",
    "                res = (y_hat - y[i])\n",
    "                if self.verbose:\n",
    "                    preds.append(abs(res)**2)\n",
    "\n",
    "                b = learning_rate*regularizer\n",
    "                # update rule for w0\n",
    "                w0 = w0 - learning_rate * res - learning_rate * w0 * regularizer\n",
    "\n",
    "                for col in range(n):\n",
    "                    #index = col\n",
    "                    #if col in ind:\n",
    "                    #    val = ind[col]\n",
    "                    index, val = _ivals[col]\n",
    "                    temp = learning_rate * val * res\n",
    "                    w1[index] -= (temp + b*w1[index])\n",
    "                    for f in range(self.k):\n",
    "                        v[index, f] -= (temp * (sum3[f] - v[index, f] * val) + b*v[index, f])\n",
    "                    #else:\n",
    "                    #    w1[index] -= b*w1[index]\n",
    "                    #    for f in range(self.k):\n",
    "                    #        v[index, f] -= b*v[index, f]\n",
    "            # val_y_pred = self.predict(self.val_X)\n",
    "            # val_mse = mean_squared_error(val_y_pred, self.val_y)\n",
    "            print(\"epoch {} time {} mse {}\".format(epoch, time.perf_counter()-start_time, np.mean(preds)))\n",
    "            # print(\"val_mse {}\".format(val_mse))\n",
    "        self.w0 = w0\n",
    "        self.w1 = w1\n",
    "        self.v = v\n",
    "    \n",
    "    \n",
    "    def fit(self,X,y,val_X, val_y):\n",
    "        self.initialize_params(X, y)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.val_X = val_X\n",
    "        self.val_y = val_y\n",
    "        self.sgd(X, y)\n",
    "        return\n",
    "    \n",
    "    def predict(self, X, verbose=True):\n",
    "\n",
    "        X = np.array(X)\n",
    "        \n",
    "        w0 = self.w0\n",
    "        w1 = self.w1\n",
    "        v = self.v\n",
    "        nz = self.nz\n",
    "\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "\n",
    "        preds = []\n",
    "        for i in range(m):\n",
    "\n",
    "\n",
    "            _x = X[i, :]\n",
    "            _ivals = self.getIndexValArray(_x)\n",
    "            ind = {}\n",
    "            sum1 = 0\n",
    "            sum2 = 0\n",
    "            sum3 = np.zeros(self.k)\n",
    "\n",
    "            for col in range(n):\n",
    "\n",
    "                index, val = _ivals[col]\n",
    "                ind[index] = val\n",
    "\n",
    "                index = int(index)\n",
    "\n",
    "                sum1 += w1[index] * val\n",
    "\n",
    "\n",
    "            for f in range(self.k):\n",
    "                s1 = 0\n",
    "                s2 = 0\n",
    "                for col in range(n):\n",
    "                    index, val = _ivals[col]\n",
    "                    index = int(index)\n",
    "                    temp = v[index, f] * val\n",
    "                    s1 += temp\n",
    "                    s2 += temp*temp\n",
    "                sum3[f] = s1\n",
    "                s1 = s1*s1\n",
    "                sum2 += s1 - s2\n",
    "\n",
    "            y_hat = w0 + sum1 + 0.5*sum2\n",
    "            y_hat = max(1., y_hat)\n",
    "            y_hat = min(5., y_hat)\n",
    "            preds.append(y_hat)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def predict_top(self, user_id, at=25, remove_seen=True, return_titles = False):\n",
    "        '''Given a user_id predict its top AT items'''\n",
    "        seen_items = self.train[self.train.user_id==user_id].movie_id.values\n",
    "        unseen_items = set(self.train.movie_id.values) - set(seen_items)\n",
    "        predictions = [(item_id,self.predict_score(user_id,item_id)) for item_id in unseen_items]\n",
    "\n",
    "        sorted_predictions = sorted(predictions, key=lambda x: x[1],reverse = True)[:at]\n",
    "        if(return_titles):\n",
    "            return [self.movie_id2title[i[0]] for i in sorted_predictions]\n",
    "        return [i[0] for i in sorted_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class FM2():\n",
    "    def __init__(self,k = 40,learning_rate = 10,solver='stochastic',iterations = 100,regularizer = 0.00035,verbose = True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.solver=solver\n",
    "        self.k = k\n",
    "        self.iterations = iterations\n",
    "        self.indices = {}\n",
    "        self.regularizer = regularizer\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "    def set_indices(self,X):\n",
    "        i = 0\n",
    "        cols = {}\n",
    "        for col in X.columns:\n",
    "            cols[col] = i\n",
    "            i += 1\n",
    "\n",
    "        indices = {}\n",
    "        nz = 0\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object':\n",
    "                indices[cols[col]] = {}\n",
    "                colset = set(X[col])\n",
    "                for a in colset:\n",
    "                    indices[cols[col]][a] = nz\n",
    "                    nz += 1\n",
    "            else:\n",
    "                indices[cols[col]] = nz\n",
    "                nz += 1\n",
    "        self.indices = indices\n",
    "        self.nz = nz\n",
    "        \n",
    "    \n",
    "    def getIndexVal(self, col, val):\n",
    "        if isinstance(self.indices[col], int) or isinstance(self.indices[col], float):\n",
    "            return self.indices[col], val\n",
    "        else:\n",
    "            try:\n",
    "                return self.indices[col][val], 1.\n",
    "            except:\n",
    "                #print(col, val)\n",
    "                return 0., 0.\n",
    "            \n",
    "    def getIndexValArray(self, arr):\n",
    "        out = []\n",
    "        for i in range(len(arr)):\n",
    "            out.append(self.getIndexVal(i, arr[i]))\n",
    "        return out\n",
    "    \n",
    "    def initialize_params(self, X, y):\n",
    "        self.set_indices(X)\n",
    "        self.w0 = np.mean(y)\n",
    "        self.w1 = np.zeros(self.nz)\n",
    "        self.v = np.random.normal(scale=0.1,size=(self.nz, self.k))\n",
    "        \n",
    "    def sgd(self, X, y, verbose = True):\n",
    "\n",
    "        learning_rate = self.learning_rate\n",
    "        regularizer = self.regularizer\n",
    "        w0 = self.w0\n",
    "        w1 = self.w1\n",
    "        v = self.v\n",
    "        nz = self.nz\n",
    "\n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        #SGD\n",
    "        for epoch in range(self.iterations):\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            preds = []\n",
    "            for i in range(m):\n",
    "\n",
    "                _x = X[i, :]\n",
    "                _ivals = self.getIndexValArray(_x)\n",
    "                ind = {}\n",
    "                sum1 = 0\n",
    "                sum2 = 0\n",
    "                sum3 = np.zeros(self.k)\n",
    "\n",
    "                for col in range(n):\n",
    "                    index, val = _ivals[col]\n",
    "                    ind[index] = val\n",
    "                    sum1 += w1[index] * val\n",
    "\n",
    "                for f in range(self.k):\n",
    "                    s1 = 0.0\n",
    "                    s2 = 0.0\n",
    "                    for col in range(n):\n",
    "                        index, val = _ivals[col]\n",
    "                        temp = v[index, f] * val\n",
    "                        s1 += temp\n",
    "                        s2 += temp*temp\n",
    "                    sum3[f] = s1\n",
    "                    sum2 += s1*s1 - s2\n",
    "\n",
    "                y_hat = w0 + sum1 + 0.5*sum2\n",
    "                y_hat = max(0., y_hat)\n",
    "                y_hat = min(1., y_hat)\n",
    "                res = (y_hat - y[i])\n",
    "                if self.verbose:\n",
    "                    preds.append(abs(res)**2)\n",
    "\n",
    "                b = learning_rate*regularizer\n",
    "                # update rule for w0\n",
    "                w0 = w0 - learning_rate * res - learning_rate * w0 * regularizer\n",
    "\n",
    "                for col in range(n):\n",
    "                    #index = col\n",
    "                    #if col in ind:\n",
    "                    #    val = ind[col]\n",
    "                    index, val = _ivals[col]\n",
    "                    temp = learning_rate * val * res\n",
    "                    w1[index] -= (temp + b*w1[index])\n",
    "                    for f in range(self.k):\n",
    "                        v[index, f] -= (temp * (sum3[f] - v[index, f] * val) + b*v[index, f])\n",
    "                    #else:\n",
    "                    #    w1[index] -= b*w1[index]\n",
    "                    #    for f in range(self.k):\n",
    "                    #        v[index, f] -= b*v[index, f]\n",
    "            # val_y_pred = self.predict(self.val_X)\n",
    "            # val_mse = mean_squared_error(val_y_pred, self.val_y)\n",
    "            print(\"epoch {} time {} mse {}\".format(epoch, time.perf_counter()-start_time, np.mean(preds)))\n",
    "            # print(\"val_mse {}\".format(val_mse))\n",
    "        self.w0 = w0\n",
    "        self.w1 = w1\n",
    "        self.v = v\n",
    "    \n",
    "    \n",
    "    def fit(self,X,y,val_X, val_y):\n",
    "        self.initialize_params(X, y)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.val_X = val_X\n",
    "        self.val_y = val_y\n",
    "        self.sgd(X, y)\n",
    "        return\n",
    "    \n",
    "    def predict(self, X, verbose=True):\n",
    "\n",
    "        X = np.array(X)\n",
    "        \n",
    "        w0 = self.w0\n",
    "        w1 = self.w1\n",
    "        v = self.v\n",
    "        nz = self.nz\n",
    "\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "\n",
    "        preds = []\n",
    "        for i in range(m):\n",
    "\n",
    "\n",
    "            _x = X[i, :]\n",
    "            _ivals = self.getIndexValArray(_x)\n",
    "            ind = {}\n",
    "            sum1 = 0\n",
    "            sum2 = 0\n",
    "            sum3 = np.zeros(self.k)\n",
    "\n",
    "            for col in range(n):\n",
    "\n",
    "                index, val = _ivals[col]\n",
    "                ind[index] = val\n",
    "\n",
    "                index = int(index)\n",
    "\n",
    "                sum1 += w1[index] * val\n",
    "\n",
    "\n",
    "            for f in range(self.k):\n",
    "                s1 = 0\n",
    "                s2 = 0\n",
    "                for col in range(n):\n",
    "                    index, val = _ivals[col]\n",
    "                    index = int(index)\n",
    "                    temp = v[index, f] * val\n",
    "                    s1 += temp\n",
    "                    s2 += temp*temp\n",
    "                sum3[f] = s1\n",
    "                s1 = s1*s1\n",
    "                sum2 += s1 - s2\n",
    "\n",
    "            y_hat = w0 + sum1 + 0.5*sum2\n",
    "            y_hat = max(0., y_hat)\n",
    "            y_hat = min(1., y_hat)\n",
    "            preds.append(y_hat)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def predict_top(self, user_id, at=25, remove_seen=True, return_titles = False):\n",
    "        '''Given a user_id predict its top AT items'''\n",
    "        seen_items = self.train[self.train.user_id==user_id].movie_id.values\n",
    "        unseen_items = set(self.train.movie_id.values) - set(seen_items)\n",
    "        predictions = [(item_id,self.predict_score(user_id,item_id)) for item_id in unseen_items]\n",
    "\n",
    "        sorted_predictions = sorted(predictions, key=lambda x: x[1],reverse = True)[:at]\n",
    "        if(return_titles):\n",
    "            return [self.movie_id2title[i[0]] for i in sorted_predictions]\n",
    "        return [i[0] for i in sorted_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 time 17.1722399580176 mse 0.19725081616280105\n",
      "epoch 1 time 16.66016824997496 mse 0.18532614124266103\n",
      "epoch 2 time 16.69018120795954 mse 0.18294891726401713\n",
      "epoch 3 time 16.596537875011563 mse 0.18170667298644508\n",
      "epoch 4 time 16.307179166993592 mse 0.18086663804929667\n",
      "epoch 5 time 16.747880542010535 mse 0.1801984970045898\n",
      "epoch 6 time 16.469946333032567 mse 0.17959966920219372\n",
      "epoch 7 time 16.616040332999546 mse 0.17901114527003023\n",
      "epoch 8 time 16.923137000005227 mse 0.17839096858686596\n",
      "epoch 9 time 16.333649959007744 mse 0.17770447195995234\n",
      "epoch 10 time 16.559552500024438 mse 0.17692290345718067\n",
      "epoch 11 time 16.527180291013792 mse 0.176026867371397\n",
      "epoch 12 time 16.72694116702769 mse 0.1750140246267897\n",
      "epoch 13 time 16.334408207971137 mse 0.17390307900474783\n",
      "epoch 14 time 16.61104929097928 mse 0.17272960162717238\n",
      "epoch 15 time 16.68842283298727 mse 0.17153211368458798\n",
      "epoch 16 time 16.376388749980833 mse 0.17034195572174968\n",
      "epoch 17 time 16.636651249951683 mse 0.1691772551158674\n",
      "epoch 18 time 16.724681042018346 mse 0.16804524343315655\n",
      "epoch 19 time 16.521416375006083 mse 0.16694750864224675\n",
      "epoch 20 time 16.474373208009638 mse 0.16588368714514154\n",
      "epoch 21 time 16.443322209001053 mse 0.16485333483075146\n",
      "epoch 22 time 16.647969542013016 mse 0.16385680151307447\n",
      "epoch 23 time 16.360831082973164 mse 0.16289473756302214\n",
      "epoch 24 time 16.581354541005567 mse 0.16196786141384104\n",
      "epoch 25 time 16.953894582984503 mse 0.16107665046625297\n",
      "epoch 26 time 16.74047379201511 mse 0.16022141064243317\n",
      "epoch 27 time 16.519240416993853 mse 0.15940237294410517\n",
      "epoch 28 time 16.516949750017375 mse 0.1586190351752963\n",
      "epoch 29 time 16.62166512501426 mse 0.15787087552332002\n"
     ]
    }
   ],
   "source": [
    "f = FM2(k=10, iterations = 30, learning_rate = 0.01, regularizer=0.03)\n",
    "f.fit(X=train, y=y_train, val_X=val, val_y=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.42520527025553695\n",
      "MSE: 0.18079952185308423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = f.predict(val)\n",
    "rmse = compute_rmse(y_pred, y_val)\n",
    "mse = mean_squared_error(y_pred, y_val)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18027390775880422,\n",
       " 0.5793209446538576,\n",
       " 0.4512330635157794,\n",
       " 0.9971136771805889,\n",
       " 0.9927257285949418,\n",
       " 0.9920305870391475,\n",
       " 0.5725321913545652,\n",
       " 0.4426281479896615,\n",
       " 0.7995001680441364,\n",
       " 0.6515089090107471,\n",
       " 0.6907856573620962,\n",
       " 0.7360195698719851,\n",
       " 0.816751595968734,\n",
       " 0.40456607445647846,\n",
       " 0.13662107973988846,\n",
       " 0.4844584228053082,\n",
       " 0.7133174983818873,\n",
       " 1.0,\n",
       " 0.3977492139843588,\n",
       " 0.3961112395866728,\n",
       " 0.7612105310273278,\n",
       " 0.16707576594812187,\n",
       " 0.3523852151797844,\n",
       " 0.6930651935229831,\n",
       " 1.0,\n",
       " 0.45175185866311013,\n",
       " 0.6986217114795436,\n",
       " 0.9168770968974347,\n",
       " 0.6617094469542847,\n",
       " 0.5716841825823332,\n",
       " 1.0,\n",
       " 0.5331630398837045,\n",
       " 0.764043364123032,\n",
       " 0.48265369000762065,\n",
       " 0.324921625474917,\n",
       " 0.9050351455563869,\n",
       " 0.35289463985224845,\n",
       " 0.6462308623538369,\n",
       " 0.2228222306930103,\n",
       " 0.25450375651734464,\n",
       " 0.9650099267188638,\n",
       " 0.5727334323768061,\n",
       " 0.0,\n",
       " 0.8248204124394506,\n",
       " 1.0,\n",
       " 0.8510429061984444,\n",
       " 0.7969497422189235,\n",
       " 0.3747933537514979,\n",
       " 0.4747523826358122,\n",
       " 0.6333854920041821,\n",
       " 0.22098256820292939,\n",
       " 0.988760640151918,\n",
       " 0.5106558162509468,\n",
       " 0.3211092301072913,\n",
       " 0.707952832687444,\n",
       " 0.7455975474156342,\n",
       " 0.538505867506693,\n",
       " 0.5858433579906934,\n",
       " 0.44879232541555303,\n",
       " 0.4562612758033254,\n",
       " 1.0,\n",
       " 0.8934612550228606,\n",
       " 0.46034881524185933,\n",
       " 0.5684298212793697,\n",
       " 1.0,\n",
       " 0.44550511880046917,\n",
       " 0.6759457316299811,\n",
       " 0.7257911808167773,\n",
       " 0.9903146119131413,\n",
       " 0.8869429717992675,\n",
       " 0.6985477491108485,\n",
       " 0.8757754366870798,\n",
       " 0.6740927446128637,\n",
       " 0.5148354792453175,\n",
       " 0.1434101704135598,\n",
       " 0.8992947304594519,\n",
       " 0.27147422835962987,\n",
       " 0.8274970379040165,\n",
       " 0.5024151884458524,\n",
       " 0.6699204959883225,\n",
       " 0.8447947564094828,\n",
       " 0.6740698223712499,\n",
       " 0.7049846753435787,\n",
       " 0.7241884377450531,\n",
       " 0.8685172871666796,\n",
       " 0.8791475343108238,\n",
       " 0.1047402981753458,\n",
       " 0.33055040713349715,\n",
       " 0.426154118002645,\n",
       " 0.6169893280448647,\n",
       " 0.044910962942879336,\n",
       " 0.7332503469840461,\n",
       " 0.7789687372854268,\n",
       " 0.8014120652377692,\n",
       " 0.17414948938548647,\n",
       " 0.8073615335602352,\n",
       " 0.6322150868475488,\n",
       " 0.24994384210773998,\n",
       " 0.6802090717295768,\n",
       " 1.0,\n",
       " 0.2321830341432619,\n",
       " 0.33808444069781873,\n",
       " 0.5837896489293602,\n",
       " 0.8798331459402969,\n",
       " 0.620892741453739,\n",
       " 1.0,\n",
       " 0.2582181561581535,\n",
       " 0.6104383483613119,\n",
       " 0.3884533821973587,\n",
       " 0.41935790061179123,\n",
       " 0.9195998814245148,\n",
       " 0.7834196166127535,\n",
       " 0.8452693156287153,\n",
       " 0.8270974865644881,\n",
       " 0.3154961161408586,\n",
       " 0.2112679363452999,\n",
       " 0.03437232231547883,\n",
       " 0.47739052022095835,\n",
       " 0.9943710794214979,\n",
       " 0.3495874540744652,\n",
       " 0.5886140893565597,\n",
       " 1.0,\n",
       " 0.3537131308166147,\n",
       " 0.6815200440258355,\n",
       " 0.9780085340937137,\n",
       " 0.8190836184709205,\n",
       " 0.813711134262898,\n",
       " 0.7262543660100936,\n",
       " 0.475933370826802,\n",
       " 0.561918898929881,\n",
       " 0.5104186441619806,\n",
       " 0.8885161449780598,\n",
       " 0.3351781987386865,\n",
       " 0.4308808392522078,\n",
       " 0.8786633196850724,\n",
       " 0.8373353623532582,\n",
       " 0.48674380542269724,\n",
       " 0.6071076459286123,\n",
       " 0.55795064633681,\n",
       " 0.5241134739794597,\n",
       " 0.7305158513277448,\n",
       " 1.0,\n",
       " 0.8267347139915947,\n",
       " 0.8482453459384991,\n",
       " 0.034642969771709824,\n",
       " 0.05485054573061318,\n",
       " 0.44076846848443807,\n",
       " 0.6930787814206081,\n",
       " 0.7582672084190123,\n",
       " 0.7331143945436371,\n",
       " 0.579459544936603,\n",
       " 0.5744272884200512,\n",
       " 0.43454976939841,\n",
       " 0.7399375546744396,\n",
       " 0.5478664256149007,\n",
       " 0.44141119023223596,\n",
       " 0.8900250502743536,\n",
       " 1.0,\n",
       " 0.5635275943088105,\n",
       " 0.265462050531386,\n",
       " 0.87570904629132,\n",
       " 0.475073507412954,\n",
       " 0.9499333854823152,\n",
       " 0.7247868110746795,\n",
       " 0.9628975093699791,\n",
       " 0.36709698364583,\n",
       " 0.7481976192331268,\n",
       " 0.5111313813003477,\n",
       " 0.9710187850336555,\n",
       " 1.0,\n",
       " 0.4395572127556649,\n",
       " 0.17616832556601586,\n",
       " 0.5139403505496648,\n",
       " 0.0,\n",
       " 0.5790144218213953,\n",
       " 0.0,\n",
       " 0.8117666014388047,\n",
       " 0.771629401011065,\n",
       " 0.7826612828226265,\n",
       " 0.7016366739706131,\n",
       " 0.2191500468352267,\n",
       " 0.46561636005331886,\n",
       " 0.8036446529461541,\n",
       " 0.402039704132119,\n",
       " 0.5042684372074776,\n",
       " 0.7900649954176275,\n",
       " 0.19834625893680086,\n",
       " 1.0,\n",
       " 0.5773121947317841,\n",
       " 0.2808105036669848,\n",
       " 0.22217078706300877,\n",
       " 0.34859263900603876,\n",
       " 0.525998535885839,\n",
       " 1.0,\n",
       " 0.14327153092692874,\n",
       " 0.34684279379211574,\n",
       " 0.7201831365029028,\n",
       " 0.7067035171185727,\n",
       " 0.4800476225338156,\n",
       " 0.6459694436659063,\n",
       " 0.2509331689739908,\n",
       " 1.0,\n",
       " 0.5486691037369462,\n",
       " 0.7642798082386097,\n",
       " 0.5395494972373904,\n",
       " 0.9161331741105262,\n",
       " 0.383654040317783,\n",
       " 0.2805626361295249,\n",
       " 0.5214909969107319,\n",
       " 0.7851061279274129,\n",
       " 0.7348929720498227,\n",
       " 0.3818104662076349,\n",
       " 0.8173243707113446,\n",
       " 0.9992435399146796,\n",
       " 0.6375654930415654,\n",
       " 1.0,\n",
       " 0.7911463565037952,\n",
       " 0.5900525552722664,\n",
       " 0.5629306624372502,\n",
       " 0.6270450716336442,\n",
       " 0.8966079048303296,\n",
       " 0.8765136046627215,\n",
       " 0.39047223541136056,\n",
       " 0.6195278032258834,\n",
       " 1.0,\n",
       " 0.7002903726364631,\n",
       " 0.3445870987684586,\n",
       " 0.9114076524478231,\n",
       " 0.8721275494766678,\n",
       " 0.8183834929824924,\n",
       " 0.6085538117275824,\n",
       " 0.6305822049744483,\n",
       " 0.7975928745990246,\n",
       " 0.9453996256134416,\n",
       " 0.9072493552864798,\n",
       " 0.47061400711610235,\n",
       " 0.8682654486959324,\n",
       " 0.448588077902899,\n",
       " 0.8277087461099022,\n",
       " 0.2545081242491657,\n",
       " 0.6653245422711059,\n",
       " 0.4207159396693685,\n",
       " 1.0,\n",
       " 0.5424020397338422,\n",
       " 0.7067013021177402,\n",
       " 0.21193797199540712,\n",
       " 0.7373868609374862,\n",
       " 0.5733952953978196,\n",
       " 0.12681168384753122,\n",
       " 0.9178067902985282,\n",
       " 0.9542583031286384,\n",
       " 0.7953915591829892,\n",
       " 0.8037166888592014,\n",
       " 0.5949691221998322,\n",
       " 0.7217641452153247,\n",
       " 0.9690040192726981,\n",
       " 0.8476584558744961,\n",
       " 0.6217233427387274,\n",
       " 0.952057466163757,\n",
       " 0.3731812545942022,\n",
       " 0.6152327649448353,\n",
       " 0.0665672231037119,\n",
       " 0.6916047811720396,\n",
       " 0.8817026727894917,\n",
       " 0.5009377355889802,\n",
       " 0.6219765439488925,\n",
       " 0.5856797872111051,\n",
       " 0.7082417631197228,\n",
       " 0.7246881444940259,\n",
       " 0.3985094871446525,\n",
       " 0.8376409582021279,\n",
       " 0.5617122391026621,\n",
       " 0.49590065208617057,\n",
       " 0.0,\n",
       " 0.7497293121721479,\n",
       " 0.5703380024323652,\n",
       " 0.7067845002996869,\n",
       " 0.043278344129375125,\n",
       " 0.5766876456302348,\n",
       " 0.9188917824048664,\n",
       " 0.7149902048287828,\n",
       " 0.870127262114591,\n",
       " 0.38710767340330915,\n",
       " 1.0,\n",
       " 0.9520547044674336,\n",
       " 0.6026052423779698,\n",
       " 0.3961422049243712,\n",
       " 0.8804295481789618,\n",
       " 0.9090656293091242,\n",
       " 0.3442675868002092,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.040044281870884164,\n",
       " 0.9787515470025812,\n",
       " 1.0,\n",
       " 0.7266466059815448,\n",
       " 0.35259126033197985,\n",
       " 0.195367859231369,\n",
       " 0.5539212645398063,\n",
       " 0.7574163476707142,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.05015515742930145,\n",
       " 0.16918248003401343,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.34130213292002526,\n",
       " 0.30253820261165365,\n",
       " 0.1667209727317676,\n",
       " 0.7951897684399313,\n",
       " 0.9150404904549686,\n",
       " 0.23473638564428503,\n",
       " 0.2529302850675291,\n",
       " 0.940682851042157,\n",
       " 0.1451711012163749,\n",
       " 0.5246437801382492,\n",
       " 0.3663950971110767,\n",
       " 0.569930554809039,\n",
       " 0.8049326854648646,\n",
       " 0.5722309311883793,\n",
       " 0.14069108019550589,\n",
       " 0.5881120172511788,\n",
       " 0.9318176415197325,\n",
       " 0.4259172053884522,\n",
       " 1.0,\n",
       " 0.33490121598426537,\n",
       " 0.8096624067864223,\n",
       " 0.9487070893302871,\n",
       " 0.5241987245001924,\n",
       " 0.9810204683781535,\n",
       " 0.32628307002304163,\n",
       " 0.31013129275669254,\n",
       " 0.8671757668175863,\n",
       " 0.5824486138745998,\n",
       " 0.5979069497143362,\n",
       " 0.6868613842548621,\n",
       " 0.8061414982517119,\n",
       " 0.3326245458048147,\n",
       " 0.7865417831909711,\n",
       " 0.6578037444869813,\n",
       " 0.0,\n",
       " 0.05135337994378156,\n",
       " 0.8686570084568014,\n",
       " 0.7068659834076594,\n",
       " 0.8906273553528214,\n",
       " 0.8725585013923336,\n",
       " 0.5474017042984838,\n",
       " 0.4883825898814207,\n",
       " 0.5793841255988136,\n",
       " 0.678417880409732,\n",
       " 0.12255396052319376,\n",
       " 0.9638939673598583,\n",
       " 0.0,\n",
       " 0.5274152180676114,\n",
       " 0.44193300190561097,\n",
       " 0.927755029156727,\n",
       " 0.9546662579093264,\n",
       " 0.21921094533054025,\n",
       " 0.5540159359488163,\n",
       " 0.9729938667429112,\n",
       " 0.3472981221019803,\n",
       " 0.39341085965839506,\n",
       " 0.7778952094519694,\n",
       " 0.16313494581662666,\n",
       " 0.5243953918532489,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.27011837869429367,\n",
       " 1.0,\n",
       " 0.5103746558516872,\n",
       " 0.26967679203495937,\n",
       " 1.0,\n",
       " 0.9002867007648887,\n",
       " 1.0,\n",
       " 0.6623351833744944,\n",
       " 0.9883297171716854,\n",
       " 0.7011532154792388,\n",
       " 0.5121005229869127,\n",
       " 0.37395363143355065,\n",
       " 0.2893416968557645,\n",
       " 0.9070597396638941,\n",
       " 0.5608228011599986,\n",
       " 0.4025846635862569,\n",
       " 1.0,\n",
       " 0.6197920480385101,\n",
       " 0.7445477090214017,\n",
       " 0.9881969951852969,\n",
       " 0.7157026662542019,\n",
       " 0.8353243687770507,\n",
       " 0.6869918266724477,\n",
       " 0.6062215845668072,\n",
       " 1.0,\n",
       " 0.9957177211615663,\n",
       " 0.7553735918027581,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.39480422518903546,\n",
       " 0.5309619761982933,\n",
       " 0.7786421292623311,\n",
       " 0.7463029522527742,\n",
       " 0.3553889039687976,\n",
       " 0.47449396009250977,\n",
       " 0.8101274236392226,\n",
       " 0.25692620289823637,\n",
       " 0.45053621552023254,\n",
       " 0.42114537031947197,\n",
       " 0.005091479715454317,\n",
       " 0.7349386589798024,\n",
       " 0.3701992512232186,\n",
       " 0.5636650017424151,\n",
       " 0.0,\n",
       " 0.9192231640243675,\n",
       " 0.7540464375995006,\n",
       " 0.8450957823098437,\n",
       " 0.8323679066655618,\n",
       " 0.44782721384281443,\n",
       " 0.562788897879589,\n",
       " 0.6882469599528314,\n",
       " 0.14781875197241345,\n",
       " 0.8030125864518555,\n",
       " 0.8856799444808261,\n",
       " 0.6626858172068959,\n",
       " 0.5112391439424685,\n",
       " 1.0,\n",
       " 0.4444509909339218,\n",
       " 0.757816983470083,\n",
       " 0.3707484599051104,\n",
       " 0.36345559074687855,\n",
       " 0.5345719895347848,\n",
       " 1.0,\n",
       " 0.6318202013253815,\n",
       " 1.0,\n",
       " 0.5209021237643829,\n",
       " 0.3292513533284653,\n",
       " 0.7348986813241704,\n",
       " 0.8052521682663425,\n",
       " 0.8117375940496164,\n",
       " 0.7068577975491116,\n",
       " 0.27642815680484795,\n",
       " 0.2727560256304712,\n",
       " 0.6649554887514247,\n",
       " 0.8262672503787385,\n",
       " 0.5577809364018119,\n",
       " 0.8030601551551858,\n",
       " 0.9428362081962167,\n",
       " 0.7739850124452122,\n",
       " 0.7972617383391826,\n",
       " 0.6796682946130959,\n",
       " 0.8108990354316321,\n",
       " 0.6204093874294269,\n",
       " 0.4376186689253234,\n",
       " 1.0,\n",
       " 0.959640286002455,\n",
       " 0.6865498915570918,\n",
       " 0.9574350778283983,\n",
       " 0.5367890748257363,\n",
       " 0.9500354345082209,\n",
       " 0.782513469659736,\n",
       " 0.39892175050266065,\n",
       " 0.859668676074461,\n",
       " 0.4810493328472246,\n",
       " 0.7949963856483637,\n",
       " 0.8984076770738905,\n",
       " 0.8688155340870102,\n",
       " 0.836074544357281,\n",
       " 0.34692921972882423,\n",
       " 1.0,\n",
       " 0.006593899170570804,\n",
       " 0.8884602338046345,\n",
       " 0.5745474661429953,\n",
       " 0.25189367292061504,\n",
       " 0.7195063634874674,\n",
       " 0.8541931223643093,\n",
       " 0.920071739301107,\n",
       " 0.6409551437870719,\n",
       " 0.7565681343618317,\n",
       " 0.21792427819582255,\n",
       " 0.12394645347134586,\n",
       " 0.259208158971914,\n",
       " 0.7254006025103159,\n",
       " 0.2779106458166918,\n",
       " 0.870289990376532,\n",
       " 0.44198375904591447,\n",
       " 0.5465464548437279,\n",
       " 0.900950652595269,\n",
       " 0.3320952891480736,\n",
       " 0.6023128079617416,\n",
       " 0.8428908590946405,\n",
       " 1.0,\n",
       " 0.10859029348262793,\n",
       " 0.5254122216462697,\n",
       " 0.8462337998346973,\n",
       " 0.2959014443642342,\n",
       " 0.36247146395218266,\n",
       " 0.7801540436132086,\n",
       " 0.6700274314872356,\n",
       " 0.8070492483575202,\n",
       " 0.7846661758730726,\n",
       " 0.760740583333722,\n",
       " 0.9794505367057793,\n",
       " 0.6032047597921012,\n",
       " 0.12480046793149532,\n",
       " 0.5786333297004982,\n",
       " 0.7732256185209179,\n",
       " 1.0,\n",
       " 0.5486871830105813,\n",
       " 0.7014427132283165,\n",
       " 0.08492825786015187,\n",
       " 0.797170552508535,\n",
       " 0.6287617712373406,\n",
       " 0.12888815819694513,\n",
       " 0.7420015664976887,\n",
       " 0.9518968732255522,\n",
       " 1.0,\n",
       " 0.8068701894377829,\n",
       " 0.768327816545914,\n",
       " 0.8789694879701407,\n",
       " 0.3526827171341495,\n",
       " 0.9479792666459785,\n",
       " 0.6028520031820739,\n",
       " 0.36130685166460896,\n",
       " 0.5611243285974487,\n",
       " 0.17233748571640797,\n",
       " 0.34475430886752295,\n",
       " 0.4607926376841098,\n",
       " 0.3790245586378049,\n",
       " 0.8185568186829311,\n",
       " 0.6800845487879098,\n",
       " 0.35616537854519253,\n",
       " 0.3587553166526931,\n",
       " 0.7276919356754046,\n",
       " 0.5913488524586514,\n",
       " 0.7913839294706744,\n",
       " 0.18138638512743052,\n",
       " 0.31572788016190634,\n",
       " 1.0,\n",
       " 0.6117840550114703,\n",
       " 0.9468788351264809,\n",
       " 0.7341783578204876,\n",
       " 0.8501427378130827,\n",
       " 0.6345097982388784,\n",
       " 0.2322451673958177,\n",
       " 1.0,\n",
       " 0.6738915992985373,\n",
       " 0.7875678868245184,\n",
       " 0.654570659587163,\n",
       " 0.0,\n",
       " 0.611042420746181,\n",
       " 0.0,\n",
       " 0.8095477937209727,\n",
       " 0.5387980693428119,\n",
       " 0.3423026900211354,\n",
       " 0.7816859757540228,\n",
       " 0.6135065194082804,\n",
       " 0.6402953610460247,\n",
       " 1.0,\n",
       " 0.17975546013684945,\n",
       " 0.7082849237991911,\n",
       " 0.4776406950251772,\n",
       " 0.27274860658433653,\n",
       " 0.818681961637531,\n",
       " 0.8913126887826868,\n",
       " 0.008660316601958623,\n",
       " 1.0,\n",
       " 0.321249555235153,\n",
       " 0.8688121501531183,\n",
       " 0.11537729333801072,\n",
       " 0.0,\n",
       " 0.5708176251610657,\n",
       " 0.917853321342953,\n",
       " 0.9063171604559123,\n",
       " 0.6476980772629644,\n",
       " 1.0,\n",
       " 0.6845810814242338,\n",
       " 0.8233628693185208,\n",
       " 1.0,\n",
       " 0.40273019869904103,\n",
       " 1.0,\n",
       " 0.246452782389049,\n",
       " 0.6709332590512505,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.4427939571643502,\n",
       " 0.9183224499897625,\n",
       " 0.4395628231843516,\n",
       " 0.10334345880777665,\n",
       " 0.7274295323948329,\n",
       " 0.8883718512249935,\n",
       " 0.2761792556042473,\n",
       " 0.2228648423941829,\n",
       " 0.45683690885512523,\n",
       " 0.6505050162478965,\n",
       " 0.2323428903017865,\n",
       " 1.0,\n",
       " 0.41815390492338395,\n",
       " 0.8282653315862049,\n",
       " 0.8584906599231656,\n",
       " 0.3924336307404438,\n",
       " 1.0,\n",
       " 0.5552952974002513,\n",
       " 1.0,\n",
       " 0.6001962380203163,\n",
       " 0.7189150491113248,\n",
       " 0.09488614734072262,\n",
       " 0.3167530761205626,\n",
       " 0.8205351616598058,\n",
       " 0.0,\n",
       " 0.9492447867730447,\n",
       " 0.3560364350603153,\n",
       " 0.7365905549062457,\n",
       " 0.23079062660045951,\n",
       " 0.5290677880703205,\n",
       " 0.12058305378462178,\n",
       " 0.3820604050242546,\n",
       " 0.6647022483122131,\n",
       " 0.6339318389828401,\n",
       " 0.03283644688823524,\n",
       " 0.883577942405874,\n",
       " 0.8161922469765617,\n",
       " 0.8492599898565103,\n",
       " 0.46591191841264135,\n",
       " 0.7068332165840717,\n",
       " 0.33180866207194626,\n",
       " 0.4501836564226745,\n",
       " 0.6663095314248719,\n",
       " 0.6461225729919199,\n",
       " 0.33884956460632715,\n",
       " 0.19038329178006816,\n",
       " 0.9525601534217027,\n",
       " 0.684107045036867,\n",
       " 0.5419253756384661,\n",
       " 0.5771215144263255,\n",
       " 0.45444895763156073,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.7601783738883794,\n",
       " 0.6844089978450255,\n",
       " 0.7026997119410339,\n",
       " 0.9273425582289448,\n",
       " 0.4780295752278598,\n",
       " 0.8747556988890219,\n",
       " 0.453810818106578,\n",
       " 1.0,\n",
       " 0.6432869232728007,\n",
       " 0.2166054582458291,\n",
       " 0.6846467944801834,\n",
       " 0.5658452339815808,\n",
       " 0.6689323197601078,\n",
       " 0.7172208875686589,\n",
       " 0.44759182633447536,\n",
       " 0.0,\n",
       " 0.6998012889295616,\n",
       " 0.7693426172400275,\n",
       " 1.0,\n",
       " 0.41134372438678235,\n",
       " 0.3252045092303747,\n",
       " 1.0,\n",
       " 0.49614484650459817,\n",
       " 0.7320773025587451,\n",
       " 0.5601572720853849,\n",
       " 0.332808342744148,\n",
       " 0.07039120716721123,\n",
       " 0.5817911147244437,\n",
       " 0.9082627939296946,\n",
       " 0.4374080248486256,\n",
       " 0.7238813326390254,\n",
       " 0.5934966852036103,\n",
       " 0.0,\n",
       " 0.22476204966845992,\n",
       " 0.656704952842304,\n",
       " 0.205714062051021,\n",
       " 0.31209884172970753,\n",
       " 0.47016110460551686,\n",
       " 0.5073575442307819,\n",
       " 0.6669712352186818,\n",
       " 0.10334562995702977,\n",
       " 0.4626233990129726,\n",
       " 0.9002526650878604,\n",
       " 1.0,\n",
       " 0.7489396984942898,\n",
       " 1.0,\n",
       " 0.38049469014943965,\n",
       " 0.4993989388676593,\n",
       " 0.9991335037681314,\n",
       " 0.14849168505896104,\n",
       " 0.40947979801901657,\n",
       " 0.6890029380805386,\n",
       " 0.0943291098576696,\n",
       " 0.6723966057442226,\n",
       " 0.6776422241101026,\n",
       " 0.5968146457874264,\n",
       " 0.12463348123195284,\n",
       " 0.3637205909340456,\n",
       " 0.6798229425207636,\n",
       " 0.12018344363883787,\n",
       " 0.8528275786791414,\n",
       " 0.8412614126106654,\n",
       " 0.2226714091481018,\n",
       " 0.5179192847817282,\n",
       " 0.8018812895786636,\n",
       " 0.7502455364063336,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8001192335244568,\n",
       " 0.7802643144092829,\n",
       " 0.7560253795409634,\n",
       " 0.9660999917340447,\n",
       " 0.0,\n",
       " 0.7687446553795311,\n",
       " 1.0,\n",
       " 0.737079066510858,\n",
       " 0.7306269444506794,\n",
       " 0.0,\n",
       " 0.24832669972591043,\n",
       " 0.690702273155644,\n",
       " 0.4912280402776545,\n",
       " 0.759222408592631,\n",
       " 1.0,\n",
       " 0.8130497312648441,\n",
       " 0.8299958597214316,\n",
       " 0.5372672322912249,\n",
       " 0.013899815898864354,\n",
       " 0.41776267268139644,\n",
       " 0.8059776039521048,\n",
       " 0.43437314548057704,\n",
       " 0.641821142011627,\n",
       " 0.6273137248942311,\n",
       " 0.532904257342464,\n",
       " 0.6797009988428433,\n",
       " 0.30528388972981874,\n",
       " 0.7385800834803865,\n",
       " 0.13167421850540476,\n",
       " 0.8028829687630032,\n",
       " 0.5486040560918061,\n",
       " 0.4436456619695883,\n",
       " 0.7653335878361103,\n",
       " 0.5310757188047284,\n",
       " 0.3368773665940784,\n",
       " 0.530799617034189,\n",
       " 0.9466736049304332,\n",
       " 0.39743955554406013,\n",
       " 0.2408220247907522,\n",
       " 0.8726662665896556,\n",
       " 0.5974058007942405,\n",
       " 0.6521327927246183,\n",
       " 0.7114755055841981,\n",
       " 0.0,\n",
       " 0.6730838493124028,\n",
       " 1.0,\n",
       " 0.4334455751783489,\n",
       " 0.7331450169633817,\n",
       " 0.1789703743733569,\n",
       " 0.8520508506095856,\n",
       " 0.871027079523225,\n",
       " 0.6102539254727418,\n",
       " 0.7836178789180637,\n",
       " 0.708086234593887,\n",
       " 1.0,\n",
       " 0.4237170741867771,\n",
       " 0.0,\n",
       " 0.13166529514082165,\n",
       " 0.9611833603745707,\n",
       " 0.376513911981181,\n",
       " 0.8589528981682102,\n",
       " 0.8929387837827496,\n",
       " 0.8975438653244603,\n",
       " 0.723960006185352,\n",
       " 0.8101625768709706,\n",
       " 0.258096996382245,\n",
       " 1.0,\n",
       " 0.9632681021690253,\n",
       " 0.0,\n",
       " 0.23160394233802237,\n",
       " 0.6299621116387227,\n",
       " 0.7733897170754451,\n",
       " 0.5817085838209216,\n",
       " 0.8145035952828407,\n",
       " 0.23771646390282894,\n",
       " 0.3038623021450547,\n",
       " 0.26947693023109054,\n",
       " 0.8230537766591776,\n",
       " 0.6088727635837214,\n",
       " 1.0,\n",
       " 0.9020040227075004,\n",
       " 0.72707949608806,\n",
       " 1.0,\n",
       " 0.4869443971224126,\n",
       " 0.055512638919617954,\n",
       " 0.6344662292115144,\n",
       " 0.7993473043858338,\n",
       " 0.48988089451099254,\n",
       " 0.7859819868210391,\n",
       " 0.512280668958535,\n",
       " 0.8533808333703334,\n",
       " 0.3738886142665478,\n",
       " 0.8570424099998604,\n",
       " 0.23698879887232863,\n",
       " 0.7193790296405145,\n",
       " 0.9345528451255327,\n",
       " 0.6358597089919332,\n",
       " 0.3549463940601113,\n",
       " 0.8342693108110014,\n",
       " 0.3268106262883601,\n",
       " 0.6540785899814962,\n",
       " 0.9671214522598172,\n",
       " 1.0,\n",
       " 0.7049286476231706,\n",
       " 0.13672348509279217,\n",
       " 0.8540554367951279,\n",
       " 0.7836599273790315,\n",
       " 0.6847614854231974,\n",
       " 0.8302716893819462,\n",
       " 1.0,\n",
       " 0.6809396738218105,\n",
       " 0.33064505044521897,\n",
       " 0.41966522465199263,\n",
       " 0.6991224555268014,\n",
       " 0.6699299730903961,\n",
       " 0.4577307451823,\n",
       " 0.8942869561248495,\n",
       " 0.6974055155629492,\n",
       " 0.7198307718623285,\n",
       " 0.5274466022581201,\n",
       " 0.8350529697512539,\n",
       " 0.5785492661609031,\n",
       " 0.1297350793718907,\n",
       " 0.7910702139611707,\n",
       " 0.906236294976942,\n",
       " 0.36962649049536855,\n",
       " 0.32517097443479015,\n",
       " 0.3905483667769628,\n",
       " 1.0,\n",
       " 0.9361947050994515,\n",
       " 0.913988408610547,\n",
       " 0.9607101102561377,\n",
       " 0.7186202646839192,\n",
       " 0.9698551010950089,\n",
       " 0.7875273640129699,\n",
       " 0.3610671367697538,\n",
       " 0.9646039631122127,\n",
       " 0.8747991362315581,\n",
       " 0.6001164260555355,\n",
       " 0.8190131362752548,\n",
       " 0.5335415701170981,\n",
       " 0.5639633932510094,\n",
       " 0.5442490793044924,\n",
       " 0.6829518461011211,\n",
       " 0.2509861301179234,\n",
       " 0.5561149259186797,\n",
       " 0.3594156390061873,\n",
       " 0.952598891137669,\n",
       " 0.4667088881839325,\n",
       " 0.4530840024950822,\n",
       " 0.45962558791425256,\n",
       " 0.7398032981339129,\n",
       " 0.8720287047388643,\n",
       " 0.6333757528227889,\n",
       " 0.3096831265735183,\n",
       " 0.6123967505682625,\n",
       " 0.5939435537631109,\n",
       " 0.9937987420090642,\n",
       " 0.7765721586154305,\n",
       " 0.5328863051589527,\n",
       " 0.8520019986216946,\n",
       " 0.0,\n",
       " 0.4319423535249652,\n",
       " 1.0,\n",
       " 0.954491429324436,\n",
       " 0.1418952267953585,\n",
       " 0.2533958048078262,\n",
       " 0.5496360418477821,\n",
       " 0.8402954149691604,\n",
       " 0.696414547003484,\n",
       " 0.5511171487535023,\n",
       " 0.9826030496626279,\n",
       " 0.7146774163016624,\n",
       " 0.9242591113659766,\n",
       " 0.29274788794596085,\n",
       " 0.49891827296081515,\n",
       " 0.6866527858898828,\n",
       " 0.10489669352827337,\n",
       " 0.8956159380834328,\n",
       " 0.25468043330446155,\n",
       " 0.8197671945334628,\n",
       " 0.44688206215954124,\n",
       " 0.6339879911168927,\n",
       " 0.37739919967754276,\n",
       " 0.9551965756452554,\n",
       " 0.24060270430809022,\n",
       " 0.7822678527321206,\n",
       " 0.7591947879193468,\n",
       " 0.9005681165398586,\n",
       " 0.7848291079250037,\n",
       " 0.619248198590862,\n",
       " 0.04267970291629272,\n",
       " 0.8597234627859955,\n",
       " 1.0,\n",
       " 0.9383346944014436,\n",
       " 0.03591052160031116,\n",
       " 0.5900251967623179,\n",
       " 0.6846314289331477,\n",
       " 0.9171283088345166,\n",
       " 0.39060248201691883,\n",
       " 0.22870503537610243,\n",
       " 0.7258736537088029,\n",
       " 0.9596479061512482,\n",
       " 0.7447593403662904,\n",
       " 0.7767341607612884,\n",
       " 0.51844764439217,\n",
       " 1.0,\n",
       " 0.35648561362008746,\n",
       " 0.8279513540496151,\n",
       " 0.8426036548975728,\n",
       " 0.5925421345894942,\n",
       " 0.32577139276055583,\n",
       " 0.7629574990080615,\n",
       " 0.59092340839506,\n",
       " 0.9243851116560626,\n",
       " 0.8998583291243576,\n",
       " 0.08732662443702965,\n",
       " 0.281083019148539,\n",
       " 0.7811981508998316,\n",
       " 0.6601555928884362,\n",
       " 0.6366888446489568,\n",
       " 0.8814287577366275,\n",
       " 0.33005024982309494,\n",
       " 0.9636490374816993,\n",
       " 0.6337907129150377,\n",
       " 0.8166826716655307,\n",
       " 0.5167516822982589,\n",
       " 0.7321809601379667,\n",
       " 0.9299720668826958,\n",
       " 0.962969228613668,\n",
       " 0.40338923982180364,\n",
       " 0.17099208950487282,\n",
       " 0.08705438481130054,\n",
       " 1.0,\n",
       " 0.8948279472633086,\n",
       " 0.4803688151839538,\n",
       " 0.7773044362531413,\n",
       " 0.7890663715069894,\n",
       " 0.40491222746016314,\n",
       " 0.4178035438932064,\n",
       " 0.347051835522492,\n",
       " 0.7892576973077627,\n",
       " 0.9190209102399778,\n",
       " 0.9990506439495829,\n",
       " 0.7437959302083792,\n",
       " 0.41351112667439616,\n",
       " 0.8350417561537482,\n",
       " 0.0072194855928762805,\n",
       " 0.9448150973781575,\n",
       " 0.32947869428152815,\n",
       " 0.5185259704405856,\n",
       " 0.2554864621668669,\n",
       " 0.6748770031327126,\n",
       " 0.4359662021905875,\n",
       " 1.0,\n",
       " 0.9168404960804772,\n",
       " 0.48971948042004104,\n",
       " 0.0,\n",
       " 0.288621440511408,\n",
       " 0.43862885928771705,\n",
       " 0.3110340294173742,\n",
       " 0.946007912259416,\n",
       " 0.29717841344631135,\n",
       " 1.0,\n",
       " 0.2896695012304807,\n",
       " 0.526667775957004,\n",
       " 0.7154665098708818,\n",
       " 0.23743512081581236,\n",
       " 0.13518777152250042,\n",
       " 0.19699842998442613,\n",
       " 0.6986689156835546,\n",
       " 0.5945930620911735,\n",
       " 0.4490483242163218,\n",
       " 0.5738601892100155,\n",
       " 0.059544344602629834,\n",
       " 0.689690537779696,\n",
       " 0.725262416183056,\n",
       " 0.8735810279839421,\n",
       " 0.7326563957957145,\n",
       " 0.9874715603107698,\n",
       " 0.10876462582381545,\n",
       " 0.8545822170264129,\n",
       " 0.8155423362183196,\n",
       " 1.0,\n",
       " 0.6190452926176423,\n",
       " 0.7021085826427849,\n",
       " 0.4614943040361178,\n",
       " 1.0,\n",
       " 0.4389688658092661,\n",
       " 0.7934502217731717,\n",
       " 0.756213834749343,\n",
       " 0.11594108616345453,\n",
       " 0.9924973640591054,\n",
       " 0.653080514499593,\n",
       " 0.7288486350460865,\n",
       " 0.19733987844528172,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanni-FM (60ite): 0.80 (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_top\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 195\u001b[0m, in \u001b[0;36mFM.predict_top\u001b[0;34m(self, df, user_id, at, remove_seen)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_top\u001b[39m(\u001b[38;5;28mself\u001b[39m,df, user_id, at\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, remove_seen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Given a user_id predict its top AT items'''\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     seen_items \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39muser_id]\u001b[38;5;241m.\u001b[39mmovie_id\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    196\u001b[0m     unseen_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(seen_items)\n\u001b[1;32m    198\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [(item_id,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_score(user_id,item_id)) \u001b[38;5;28;01mfor\u001b[39;00m item_id \u001b[38;5;129;01min\u001b[39;00m unseen_items]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#f.predict_top(1, df, at=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
